# -*- coding: utf-8 -*-
"""preprocess_data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NmOuTKdFtIBk4U0xtqZARBQTRmTIm79j
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

coinbase_path = '/content/coinbaseUSD_1-min_data_2014-12-01_to_2019-01-09.csv'
bitstamp_path = '/content/bitstampUSD_1-min_data_2012-01-01_to_2020-04-22.csv'
df_coinbase = pd.read_csv(coinbase_path)
df_bitstamp = pd.read_csv(bitstamp_path)
# print("Coinbase Dataset: ")
# df_coinbase.head()

# print("Bitstamp Dataset: ")
# df_bitstamp.head()

df_coinbase.shape, df_bitstamp.shape

# Concat 2 datasets
df = pd.concat([df_coinbase, df_bitstamp])
# Get more information about our dataset
# df.info()
# df.describe()

df.head()

df.columns

df[df['Timestamp']==60.0]

df = df[~df['Timestamp'].isna()]

df['Timestamp'].value_counts()

df['Timestamp'].diff().value_counts()

useless_rows = df[df['Timestamp'].diff() != 60]
df = df[df['Timestamp'].diff().fillna(60) == 60]

# # Checking for nan values
# df.isna().sum()

# df.head()

df = df.sort_values('Timestamp')

df.shape, df.dropna().shape

# Let's fill nan values y forward fill
# Step 1: Sort by timestamp
# df = df.sort_values('Timestamp')

# # Step 2: Forward fill
# df.fillna(method='ffill', inplace=True)

# # Step 3: Backward fill (first row və sonda qalan NaN-lar üçün)
# df.fillna(method='bfill', inplace=True)

# Step 4: Check
# df.isna().sum()

# df.head()

# # Close price trend
# df['Close'].plot(title='BTC Close Price')
# plt.show()

# # Close vs VWAP
# df[['Close', 'Weighted_Price']].plot(title='Close vs VWAP')
# plt.show()

features = ['Open', 'High', 'Low', 'Close', 'Volume_(BTC)', 'Volume_(Currency)', 'Weighted_Price']
target_column = 'Close'

window_size = 1440
target_offset = 60
batch_size = 32

train_size = int(len(df) * 0.8)
df_train = df.iloc[:train_size].copy()
df_val   = df.iloc[train_size:].copy()

feature_scaler = StandardScaler()
train_reshaped = df_train[features].values.astype(np.float32)
feature_scaler.fit(train_reshaped)

target_scaler = StandardScaler()
target_scaler.fit(df_train[target_column].values.reshape(-1,1))

def sliding_window_generator(df, batch_size=32, is_train=True):
    n = len(df) - window_size - target_offset
    X_batch, y_batch = [], []

    for i in range(n):
        X_seq = df[features].iloc[i:i+window_size].values.astype(np.float32)
        y_target = np.float32(df[target_column].iloc[i+window_size+target_offset-1])
        X_seq_scaled = feature_scaler.transform(X_seq)
        y_target_scaled = target_scaler.transform(np.array([[y_target]])).flatten()[0]
        X_batch.append(X_seq_scaled)
        y_batch.append(y_target_scaled)

        if len(X_batch) == batch_size:
          # We use yield because we need to transfer data to the model in batches, without loading all the data into RAM
            yield np.array(X_batch), np.array(y_batch)
            X_batch, y_batch = [], []

    if len(X_batch) > 0:
        yield np.array(X_batch), np.array(y_batch)

train_dataset = tf.data.Dataset.from_generator(
    lambda: sliding_window_generator(df_train, batch_size=batch_size),
    output_types=(tf.float32, tf.float32),
    output_shapes=((None, window_size, len(features)), (None,))
).prefetch(tf.data.AUTOTUNE)

val_dataset = tf.data.Dataset.from_generator(
    lambda: sliding_window_generator(df_val, batch_size=batch_size),
    output_types=(tf.float32, tf.float32),
    output_shapes=((None, window_size, len(features)), (None,))
).prefetch(tf.data.AUTOTUNE)

# print("Train and validation datasets ready!")